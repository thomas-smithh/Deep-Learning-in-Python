{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 - Fundamentals of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glossary:\n",
    "- Batch: A set of samples that are processed simulaneously by the model. The number of samples is typically a power of 2, so facillitate memory allocation on the GPU. When training, a mini-batch is used to compute a single gradient-descent update aplied to the weights of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Data preprocessing for neural networks\n",
    "\n",
    "- Vectorization: All inputs and targets in neural networks must be tensors of floating-point data. Whatever data is processed, you must first turn into tensors, a step called data vectorization.\n",
    "- Value Normalization: In general, it isn't safe to feed into a neural network data that takes relatively large values or data that is heterogeneous. Doing so can trigger large gradient updates that will prevent the network from converging. Typically data should take small values and be homogeneous.\n",
    "- Handling missing values: If you're expecting missing values in the test data, copy some samples of the training data, replacing the values you expect to be missing with 0s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 Adding weight regularization\n",
    "\n",
    "Simple models are elss likely to overfit than complex ones. A simple model in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters). A common way to mitigate overfitting is to put constraints on the complexity of the network by forcing its weights to take only small values, which makes the distribution of weights more _regular_. It's done by adding to the loss function of the network a cost associated with having large weights. This comes in two flavours:\n",
    "- L1 regularization - The cost added is proportional to the absolute value of the weight coefficients.\n",
    "- L2 regularization - The cost added is proportional to the square of the value of the weight coefficients. L2 normalization is also called weight decay int the context of neural networks. \n",
    "\n",
    "In keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. Let's add L2 weight regularization to the movie-review classification network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000, )))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2(0.001) means that every coefficient in the weight matrix will add 0.001 * _weight_coefficient_value_ to the total loss of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3 Adding dropout\n",
    "\n",
    "_Dropout_ is one of the most effective and most commonly used regularization techniques for neural networks. Dropout, applied to a layer, consists of randomly setting to zero a number of output features of the layer during training. The dropout rate is the fraction of features that are zeroed out. At test time, no units are dropped out; instead, the layer's output values are scaled down by a factor equal to the dropout rate, to balance for the fact that more units are active than at training time. The core idea is that introducing noise in the output of a layer can break up happenstance patterns that aren't significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, these are the most common ways to prevent overfitting neural networks:\n",
    "- Get more training data\n",
    "- Reduce the capacity of the network\n",
    "- Add weight regularization\n",
    "- Add dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter Summary\n",
    "\n",
    "- Define the problem at hand and the data on which you'll train. Collect this data, or annotate it with labels if need be.\n",
    "- Choose how you'll measure success on your problem. Which metrics will you monitor on your validation data?\n",
    "- Determine your evaluation protocol: hold-out validation? K-fold validation? Which portion of the data should you use for validation?\n",
    "- Develop a model that does better than a basic baseline: a model with statistical power.\n",
    "- Develop a model that overfitws\n",
    "- Regularize your model and tune its hyperparameters, based on performance on the validation data. A lot of ML research tend to focus only on this step - but keep the big picture in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
